Part 1: Conceptual Design

**Abstract**: Medical imaging plays a critical role in diagnosing respiratory conditions, yet accurate interpretation of chest CT scans and X-rays remains challenging. Visual differences between diseases like COVID-19 and tuberculosis can be subtle, and diagnostic interpretation often varies between radiologists. This project aims to develop an automated multi-class classification system capable of distinguishing between COVID-19, tuberculosis, and normal lung cases from chest imaging data.

**Problem**: The primary challenge in chest imaging classification lies in the subtle visual distinctions between different pathologies. COVID-19 typically presents with bilateral or peripheral ground-glass opacities, consolidation in lower lobes, vascular enlargement, and crazy-paving patterns. Tuberculosis, on the other hand, manifests through upper lobe infiltrates, cavitary lesions, tree-in-bud appearance, fibrosis, and volume loss with often asymmetrical chronic scarring. Normal lungs appear fully aerated with no visible opacities, nodules, fibrosis, cavitation, or signs of infection. These differences, while clinically documented, can be difficult to distinguish visually—especially for automated systems without domain-specific training.

**Solution**: To address this classification problem, I propose a two-pronged approach that evaluates both zero-shot capabilities and fine-tuning potential of state-of-the-art (SOTA) vision-language models. First, I will establish a baseline using Qwen2.5-VL-3B's zero-shot performance to understand how well a powerful multimodal model can classify medical images without task-specific training. Second, I will fine-tune the SigLIP (Sigmoid Loss for Image-Text Pairs) architecture, which employs a Vision Transformer backbone designed for efficient contrastive learning between image and text pairs. The hypothesis is that while foundation models possess general visual understanding, domain-specific fine-tuning remains essential for optimal medical image classification accuracy.
For feature extraction, I will rely on SigLIP's Vision Transformer backbone, which processes image patches through self-attention mechanisms. This generates high-quality image embeddings that capture both fine-grained details (subtle opacities, lesion boundaries) and global context (overall lung structure, bilateral patterns) necessary for distinguishing between visually similar pathologies. The model should learn to focus on diagnostically relevant regions such as lung fields, opacity patterns, and structural abnormalities.
The solution should ideally be agnostic to several factors that vary across medical imaging datasets: scanner manufacturer and model, patient demographics (age, gender), image acquisition parameters, and minor variations in patient positioning. By training on diverse data sources, the classifier should generalize across these variations rather than overfitting to specific imaging conditions.

**Datasets**: I plan to assemble a dataset of approximately 16,300 images across three classes from publicly available medical imaging repositories:

- **COVID CT Slices Dataset**: Provides approximately 14,486 PNG-format images (7,593 COVID-positive and 6,893 normal cases). This dataset offers a large volume of labeled COVID-19 cases for training.
- **MosMedData Chest CT Scans**: Contains 3D volumetric CT scans in NIfTI format with COVID-19 related findings. I will extract center slices from the axial plane, yielding approximately 1,110 2D PNG images. Center slice extraction is chosen because this view typically contains the most comprehensive representation of lung pathology.
- **Chest X-ray Dataset for Tuberculosis**: Provides 704 PNG-format images with tuberculosis positive and negative labels. This dataset addresses the tuberculosis class, though its smaller size presents a class imbalance challenge.

All images will be standardized to 224×224 pixels to meet model input requirements. The data will be split using an 80/10/10 ratio for training, validation, and testing. The training set will be used to optimize model parameters, the validation set will monitor generalization and prevent overfitting, and the test set will remain untouched until final evaluation to provide an unbiased assessment of model performance.
Anticipated Challenges: The significant class imbalance (tuberculosis: ~700 images vs. COVID-19: ~8,400 images) will likely require weighted sampling or other balancing techniques during training. Additionally, the datasets originate from different sources with varying image characteristics, which may introduce domain shift issues that the model must learn to handle.

**Next Steps**: After data acquisition, I will implement preprocessing pipelines to standardize images across sources, evaluate zero-shot baselines, and then proceed with fine-tuning experiments to optimize classification performance.

